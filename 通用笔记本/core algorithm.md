## 数据分析核心流程

**广义的数据分析包括狭义数据分析和数据挖掘。**

-  狭义的数据分析是指**根据分析目的**，采用`对比分析、分组分析、交叉分析和回归分析等分析方法`，对收集来的数据进行处理与分析，提取有价值的信息，发挥数据的作用，得到一个**特征统计量结果**的过程。（统计量）
-  数据挖掘则是从大量的、不完全的、有噪声的、模糊的、随机的实际应用数据中，通过应用聚类、分类、回归和关联规则等技术，**挖掘潜在价值**的过程。（潜在）

<img src="core algorithm.assets/image-20231115152050393.png" alt="image-20231115152050393" style="zoom: 50%;" />



完整得数据分析流程如下图：

<img src="core algorithm.assets/image-20231115152123888.png" alt="image-20231115152123888" style="zoom:50%;" />

1.  确定需求（这很重要决定了后续的分析的方法和方向，多个部门，业务，财务，生产），建立数据链路（物联网实时、历史、爬虫）。

2.  数据读取与描述性统计分析（了解数据的基本特征和概貌）

3.  数据处理

    > 数据清洗（数据一致性、无效值，异常值，重复值以及缺失值的处理，剔除数据相关性高的数据（减少冗余 ）、标准化，数据变换分解...)
    >
    > 数据规约（删除无用数据列或生成新数据列，重采样 或 离散化&连续化（以年月日重采样，或连续数据化为区间标签），数据聚合，属性规约，数据降维、压缩等等）
    >
    > 1. 数据清理（Data Cleaning）：数据清理是数据处理中的第一步，目的是处理原始数据中的噪声、错误、缺失值和重复值等问题。数据清理的任务包括去除异常值、填补缺失值、解决数据不一致性等，以确保数据的质量和一致性。
    > 2. 数据集成（Data Integration）：数据集成是将来自不同数据源的数据进行整合的过程。在数据挖掘中，常常需要从不同的数据表、数据库或文件中获取数据，并将其合并成一个统一的数据集。数据集成的目标是解决数据源异构性、数据冗余和一致性等问题。
    > 3. 数据转换（Data(数据) Transformation(转型)）：数据转换是指对原始数据进行变换和规范化，以适应数据挖掘算法的要求。数据转换的任务包括特征选择、特征提取、数据标准化、离散化、聚集和规范化等。通过数据转换，可以提高数据的表达能力和挖掘算法的性能。
    > 4. 数据降维（Dimensionality Reduction(还原)）：数据降维是对高维数据进行处理，以减少特征的数量和复杂度。高维数据集往往会导致维度灾难和计算复杂度的增加，因此需要通过降维技术将数据映射到较低维度的空间中，同时保留数据的主要信息。
    > 5. 数据规约（Data(数据) Reduction(还原)）：数据规约是指通过抽样、聚类、离散化等技术减少数据量，以提高数据挖掘的效率和速度。数据规约可以减少计算成本，加快算法执行速度，并且在某些情况下可以保持数据挖掘的准确性。

    **狭义数据分析**：数据可视化分析 & 数据检验，数据相关性分析、对比分析... (指标统计量)  - 提取有价值信息

4.  **数据挖掘**：提取特征（特征工程），并根据数据和需求进行分类、聚类、关联性、预测、回归... 并根据评价指标进行性能优化（模型或规则）- 提取潜在信息

7.  将通过了正式应用数据分析结果与结论应用至实际生产系统（微服务部署或分析报告汇报）

### 数据分析应用场景 (确定需求)

 企业使用数据分析解决不同的问题，实际应用的数据分析场景主要分为以下7类。

#### 客户分析(CustomerAnalytics)

客户分析主要是根据客户的基本数据信息进行商业行为分析，首先界定目标客户，根据客户的需求、目标客户的性质、所处行业的特征以及客户的经济状况等基本信息，使用统计分析方法和预测验证法分析目标客户，提高销售效率。其次了解客户的采购过程，根据客户采购类型、采购性质进行分类分析，制定不同的营销策略。最后还可以根据已有的客户特征进行客户特征分析、客户忠诚度分析、客户注意力分析、客户营销分析和客户收益分析。通过有效的客户分析能够掌握客户的具体行为特征，将客户细分，使得运营策略达到最优，提升企业整体效益等

#### 营销分析(Sales and Marketing Analytics）

营销分析囊括了产品分析、价格分析、渠道分析、广告与促销分析这4类分析。产品分析主要是竞争产品分析，通过对竞争产品的分析制定自身产品策略。价格分析又可以分为成本分析和售价分析。成本分析的目的是降低不必要的成本;售价分析的目的是制定符合市场的价格。渠道分析是指对产品的销售渠道进行分析，确定最优的渠道配比。广告与促销分析则能够结合客户分析，实现销量的提升、利润的增加。

#### 社交媒体分析(Social MediaAnalytics )
社交媒体分析是以不同的社交媒体渠道生成的内容为基础，实现不同社交媒体的用户分析、访问分析和互动分析等。用户分析主要根据用户注册信息、登录平台的时间点和平时发表的内容等用户数据，分析用户个人画像和行为特征:访问分析则是通过用户平时访问的内容分析用户的兴趣爱好，进而分析潜在的商业价值;互动分析根据互相关注对象的行为预测该对象未来的某此行为特征。同时，社交媒体分析还能为情感和舆情监督提供丰富的资料。	

#### 网络安全(Cyber Security)

大规模网络安全事件的发生，例如2017年5月席卷全球的 WannaCry病毒，让企业意识到网络攻击发生时预先快速识别的重要性。传统的网络安全主要依靠静态防御，处理病毒的主要流程是发现威胁、分析威胁和处理威胁。这种情况下，往往在威胁发生以后才能做出反应。新型的病毒防御系统可使用数据分析技术，建立潜在攻击识别分析模型，监测大量网络活动数据和相应的访问行为，识别可能进行入侵的可疑模式，做到未雨绸缪。

#### 设备管理(Plant and Facility Management)

设备管理同样是企业关注的重点。设备维修一般采用标准修理法、定期修理法和检查后修理法等方法。其中，标准修理法可能会造成设备过剩修理，修理费用高;检查后修理法解决了修理费用成本问题，但是修理前的准备工作繁多，设备的停歇时间过长。目前企业能够通过物联网技术收集和分析设备上的数据流，包括连续用电、零部件温度、环境湿度和污染物颗粒等多种潜在特征，建立设备管理模型，从而预测设备故障，合理安排预防性的维护，以确保设备正常作业，降低因设备故障带来的安全风险。

#### 交通物流分析(Transport and Logistics Analytics )

物流是物品从供应地向接收地的实体流动，是将运输、储存、装卸搬运、包装、流通加工、配送和信息处理等功能有机结合起来而实现用户要求的过程。用户可以通过业务系统和GPS 定位系统获得数据，使用数据构建交通状况预测分析模型，有效预测实时路况物流状况、车流量、客流量和货物吞吐量，进而提前补货，制定库存管理策略。

#### 欺诈行为检测(Fraud Detection )

身份信息泄露及盗用事件逐年增长，随之而来的是欺诈行为和交易的增多。公安机关各大金融机构、电信部门可利用用户基本信息、用户交易信息和用户通话短信信息等数据识别可能发生的潜在欺诈交易，做到提前预防、未雨绸缪。以大型金融机构为例，通过分类模型分析方法对非法集资和洗钱的逻辑路径进行分析，找到其行为特征。聚类模型分析方法可以分析相似价格的运动模式。例如对股票进行聚类，可能发现关联交易及内幕交易的可疑信息。关联规则分析方法可以监控多个用户的关联交易行为，为发现跨账号协同的金融诈骗行为提供依据。

### 数据质量分析（初步分析）

以下是一些常见的数据质 量分析算法，以后后续的处理提供方向和思路。

| 算法名称                                         | 介绍                                                         | 优缺点                                                       |
| ------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 描述性统计分析 (Descriptive Statistics Analysis) | 描述性统计分析是通过计算和汇总数据的基本统计指标来描述数据的特征。这些统计指标包括均值、中位数、标准差、最小值、最大值、百分位数等。描述性统计分析提供了关于数据分布、集中趋势和变异程度的信息。 | 优点：提供了对数据的基本理解和总结，可以帮助发现数据中的异常情况和问题。 缺点：仅提供了数据的概括性信息，可能无法捕捉到数据中的细节和复杂关系。 |
| 缺失值分析 (Missing Value Analysis)              | 用于检测和处理数据中的缺失值。它可以帮助确定缺失值的模式和原因，并选择适当的处理方法，如删除、插值或填充缺失值。 | 优点：可以帮助恢复丢失的数据，提高数据完整性。 缺点：对于大量缺失值的数据，处理方法可能会引入误差。 |
| 异常值检测 (Outlier Detection)                   | 用于识别数据中的异常值或离群值。这些异常值可能是由于测量错误、数据录入错误或其他异常情况引起的。异常值检测可以帮助发现数据中的问题并采取适当的措施。 | 优点：可以帮助发现数据质量问题，避免异常值对后续分析的影响。 缺点：可能会将正常但罕见的观测值误分类为异常值。 |
| 一致性分析 (Consistency Analysis)                | 用于评估数据中的一致性或冲突。它可以帮助发现数据中的不一致之处，例如相同实体的不同表达方式或不一致的属性值。一致性分析可以帮助清理和整理数据，使其更加一致和可靠。 | 优点：可以发现数据中的不一致性问题，提高数据的一致性和可信度。 缺点：可能需要复杂的规则和算法来检测一致性问题。 |
| 数据重复性分析 (Data Deduplication)              | 用于检测和处理数据中的重复记录。数据重复性分析可以帮助识别和删除重复的数据，以确保数据的唯一性和一致性。 | 优点：可以提高数据的准确性和一致性，减少重复数据对分析的影响。 缺点：可能需要消耗大量的计算资源和时间，特别是对于大型数据集。 |
| 数据验证 (Data Validation)                       | 用于验证数据是否符合预期的格式、范围或规则。数据验证可以帮助发现数据中的错误或异常，并提供数据质量报告。 | 优点：可以帮助发现数据中的错误或异常，提高数据的准确性和可靠性。 缺点：可能需要定义和实施复杂的验证规则和算法。 |

#### 描述性统计分析

##### 数值型描述性统计分析

描述性统计分析是对数据进行总结和描述的方法。它包括计算数据的中心趋势（如平均值、中位数、众数）、数据的离散程度（如方差、标准差、范围）以及数据的分布形态（如直方图、箱线图）。描述性统计分析可以帮助我们了解数据的基本特征和概貌。

下面是描述性统计分析中常用的分析算法

| 名称                                           | 介绍                                                         | 优缺点                                                       |
| ---------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 频数分析<br>(Frequency Analysis)               | 频数分析是一种统计方法，用于计算和展示数据中不同值或取值范围的出现次数。它通常用于描述离散变量的分布情况。通过统计每个取值出现的频数，可以揭示数据的分布特征和重要特点。 | 优点：<br>1. 提供了对数据的直观了解，可以揭示数据的分布情况。<br>2. 对于离散数据，频数分析是一种简单有效的描述方式。<br>缺点：<br>1. 仅提供了关于频数的信息，忽略了其他统计量的分析。<br>2. 不适用于连续变量或大量取值的离散变量。 |
| 集中趋势分析<br>(Measures of Central Tendency) | 集中趋势分析用于描述数据的中心位置或平均水平。常用的集中趋势分析方法包括平均数、中位数和众数。这些方法可以帮助了解数据的典型值和分布特征。 | 优点：<br>1. 提供了对数据集中程度的度量，有助于了解数据的核心特征。<br>2. 简单易懂，计算方法直观。<br>缺点：<br>1. 平均数容易受到极端值的影响，不适合用于偏态分布的数据。<br>2. 中位数和众数无法提供关于数据分布的详细信息。 |
| 离散程度分析<br>(Measures of Dispersion)       | 离散程度分析用于衡量数据的离散程度或变异程度。常用的离散程度分析方法包括范围、方差和标准差等。这些方法可以帮助判断数据的分散程度和变异程度。 | 优点：<br>1. 提供了关于数据分散程度的度量，有助于了解数据的变异程度。<br>2. 可以用于比较不同数据集之间的离散程度。<br>缺点：<br>1. 范围只考虑了最大值和最小值，忽略了其他数据点的信息。<br>2. 方差和标准差可能受到极端值的影响，不适合用于偏态分布的数据。 |

以下则是各个统计量的表格


| 名称                        | 介绍                                                         | 特点                                                       |
| --------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- |
| 平均值 (Mean)               | 数据集中所有观测值的算术平均值。                             | 受极端值的影响较大，对异常值敏感。                         |
| 中位数 (Median)             | 数据集中按大小排列的中间值。                                 | 不受极端值的影响，对异常值较为稳健。                       |
| 众数 (Mode)                 | 数据集中出现频率最高的值。                                   | 可能存在多个众数或无众数。                                 |
| 方差 (Variance)             | 观测值与其平均值之差的平方的平均值。衡量数据的离散程度。     | 平方单位与原始数据单位不同，对极端值敏感。                 |
| 标准差 (Standard Deviation) | 方差的平方根。衡量数据的离散程度。                           | 与原始数据处于同一单位，对极端值敏感。                     |
| 百分位数 (Percentile)       | 数据集中特定百分比位置的值。                                 | 可以衡量数据的分布情况，如中位数是百分位数的一种特殊情况。 |
| 最小值 (Minimum)            | 数据集中的最小值。                                           | 描述数据集的最小观测值。                                   |
| 最大值 (Maximum)            | 数据集中的最大值。                                           | 描述数据集的最大观测值。                                   |
| 范围 (Range)                | 最大值与最小值之间的差。                                     | 衡量数据的变异范围。                                       |
| 四分位数 (Quartiles)        | 数据集被划分为四个等分的值。                                 | 可以衡量数据的分布情况，如中位数是四分位数的一种特殊情况。 |
| 偏度（Skewness）            | 偏度是描述概率分布偏斜程度的统计量。它测量了数据分布的不对称性。当偏度值为0时，数据分布是对称的；当偏度值大于0时，数据分布向右偏斜（正偏态）；当偏度值小于0时，数据分布向左偏斜（负偏态）。 | 偏度是对称性的一种度量，它可以帮助我们了解数据分布的形状。 |
| 峰度（Kurtosis）            | 峰度是描述概率分布尖峰或平坦程度的统计量。它度量了数据分布相对于正态分布的尖峰或平坦程度。正态分布的峰度为3。当峰度值大于3时，数据分布比正态分布更尖峭（尖峰态）；当峰度值小于3时，数据分布比正态分布更平坦（平峰态） | 峰度可以帮助我们判断数据分布的尖峰程度和离散程度。         |

以下则是用于输出数据质量分析的自定义代码：

```python
# 自定义analysis函数，实现数据信息探索的描述性统计分析和缺失值分析
def analysis(data):
    # 计算偏度和峰度
    skewness = data.skew()
    kurt = data.kurtosis()
    description = data.describe()
    description.loc['skewness'] = skewness
    description.loc['kurtosis'] = kurt
    # 计算每一列的重复值个数
    duplicates = data.duplicated().sum()
    duplicates_by_column = data.apply(lambda x: x.duplicated().sum()) # each
    print('数据相关信息：\n', data.info())
    print('描述性统计分析结果为：\n', description.T)
    print('各属性缺失值占比为：\n', 100*s(data.isnull().sum() / len(data)))
    print('\n 全部重复值个数: {} ,占比： {}'.format(duplicates,100*(duplicates / len(data))))
    print('各属性重复值占比为：\n', 100*(duplicates_by_column / len(data))) # 注意此时重复值很多需考虑真实场景
```

##### 类别型描述性统计分析

